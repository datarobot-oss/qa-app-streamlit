resultText_PREDICTION,DEPLOYMENT_APPROVAL_STATUS,LLM_PROVIDER_GUARD_TRIGGERED_OUTPUT,Responses_token_count_OUTPUT,blocked_resultText_OUTPUT,ROUGE-1 Guard_reported_resultText_OUTPUT,reported_resultText_OUTPUT,Responses_rouge_1_OUTPUT,action_promptText_OUTPUT,Prompts_injection_injection_PREDICTION_OUTPUT,reported_promptText_OUTPUT,Prompt Tokens_reported_promptText_OUTPUT,datarobot_confidence_score_OUTPUT,moderated_promptText_OUTPUT,Response Tokens_latency_OUTPUT,Prompt Injection Guard Configuration [RAG]_latency_OUTPUT,datarobot_latency_OUTPUT,Prompts_flagged_true_PREDICTION_OUTPUT,action_resultText_OUTPUT,_LLM_PROMPT_VECTOR_OUTPUT,replaced_resultText_OUTPUT,Response Tokens_reported_resultText_OUTPUT,replaced_promptText_OUTPUT,Prompts_token_count_OUTPUT,Prompts_toxicity_toxic_PREDICTION_OUTPUT,Toxicity Guard Configuration [RAG]_blocked_promptText_OUTPUT,Prompt Injection Guard Configuration [RAG]_blocked_promptText_OUTPUT,Keyword Guard Configuration [RAG]_latency_OUTPUT,unmoderated_resultText_OUTPUT,Toxicity Guard Configuration [RAG]_latency_OUTPUT,ROUGE-1 Guard_latency_OUTPUT,_LLM_CONTEXT_OUTPUT,blocked_promptText_OUTPUT,Prompt Tokens_latency_OUTPUT,Keyword Guard Configuration [RAG]_blocked_promptText_OUTPUT,LLM_BLUEPRINT_ID_OUTPUT,datarobot_token_count_OUTPUT,association_id
"Why did the developer go broke? Because he used up all his cache!",APPROVED,False,15,False,False,False,0.6153846154,,4.883e-06,False,False,0.0,'Tell me a joke',0.0018489361,0.3767225742,1.529995203,0.0,,"[51, 66, 110, 57, 234, 32, 226, 27, 55, 187, 190, 56, 251, 16, 247, 100, 42, 223, 39, 10, 88, 238, 219, 227, 155, 134, 54, 27, 205, 225, 134, 57, 196, 129, 196, 94, 224, 228, 140]",False,False,False,6,0.0037650669,False,False,0.1959226131,"Why did the developer go broke? Because he used up all his cache!",0.1600658894,0.0639913082,"[{""content"": ""---\ntitle: Troubleshooting the Worker Queue\ndescription: If you expect to be able to increase your worker count but cannot, check the reasons described here.\n\n---\n\n#  Troubleshooting the Worker Queue {: #troubleshooting-the-worker-queue }\n\n{% include 'includes/worker-queue-tbsht-include.md' %}"", ""link"": ""datarobot_english_documentation/datarobot_docs|en|get-started|gs-get-help|troubleshooting|workers-help.txt:"", ""vector"": [153, 12, 253, 123, 163, 173, 226, 227, 179, 23, 153, 167, 135, 216, 87, 40, 170, 200, 79, 62, 89, 78, 65, 213, 25, 103, 225, 106, 170, 245, 198, 184, 188, 223, 135, 214, 194, 109, 170], ""metadata"": {""chunk_id"": 2611, ""source"": ""datarobot_english_documentation/datarobot_docs|en|get-started|gs-get-help|troubleshooting|workers-help.txt"", ""start_index"": 0, ""similarity_score"": 2.29}}, {""content"": ""## Send comments to DataRobot Support {: #send-comments-to-datarobot-support }\n\nYou can include a screenshot with the **Ask a Question**, **Report a Bug**, or **Suggest a Feature** links. To communicate with DataRobot Support:\n\n1. From the question mark (![](images/icon-question.png)) dropdown, click on the desired link to open the corresponding modal.\n\n    ![](images/feedback-modal.png)\n\n2. Complete the fields&mdash;a category and sub-category and a description. If you selected \""Other\"" as a sub-category, you are also prompted for a classification for the issue.\n\n3. Optionally, use your screen capture software to highlight the area of focus and include a PNG, JPEG, or JPG image that provides additional information.\n\n\tNote that you may want to block areas of the screen that you do not want sent as part of the communication (sensitive data or user names, for example).\n\n4. Click **Send** to send your message to DataRobot Support. Click **Close** to cancel.\n\n\n## Report a bug without SMTP {: #report-a-bug-without-smtp }\n\nIf your deployment does not have SMTP configured, you cannot report a bug or comment through the application. Instead, you can click the provided link to open your email and report, with screenshots, via email.\n\n![](images/send-email.png)\n\nTo report without SMTP:\n\n1. Click the link to open an email addressed to support@datarobot.com. Any comments you entered are transferred to the email.\n\n2. If you included a screenshot, copy or drag the image to your email.\n\n3. Enter any additional comments or images and send the email.\n\n## Self-Managed AI Platform admins {: #self-managed-ai-platform-admins }\n\nThe following is available only on the Self-Managed AI Platform.\n\n### DataRobot license {: #datarobot-license }"", ""link"": ""datarobot_english_documentation/datarobot_docs|en|platform|account-mgmt|getting-help.txt:3"", ""vector"": [190, 45, 221, 68, 170, 152, 113, 25, 186, 175, 134, 158, 227, 81, 158, 65, 37, 239, 151, 69, 64, 199, 155, 245, 179, 47, 117, 84, 207, 212, 132, 143, 220, 129, 218, 228, 66, 28, 132], ""metadata"": {""chunk_id"": 3334, ""source"": ""datarobot_english_documentation/datarobot_docs|en|platform|account-mgmt|getting-help.txt"", ""start_index"": -1, ""similarity_score"": 2.12}}, {""content"": ""###  Area Under ROC Curve Error (AUC) {: #area-under-roc-curve-error-auc }\n\n* AUC is a common error metric for classification and works by optimizing the ability of a model to separate the 1s from the 0s. AUC is not sensitive to the relative number of 0s and 1s in the target variable and can be a good choice for skewed classes. When optimized for AUC, predicted values will effectively order inputs from the most to least likely to be 1; however, they _cannot_ be interpreted as a predicted probability.\n\n## Error metrics and noise {: #error-metrics-and-noise }\n\nOne consideration for choosing an error metric is the expected amount of noise in the data. Different error metrics effectively make different assumptions about the distribution of the noise in the observed output. For example, for very noisy systems you might select an error metric that would give relatively less weight to some large errors (e.g., Mean Absolute Error, IQR Error, Median Error) under the assumption that these large errors may be due to noise in the input data rather than poor model fit. On the contrary, when input data is expected to have very low noise you might select an error metric which heavily penalizes large errors (e.g., R^2 or Maximum Error).\n\n###  Noisy systems {: #noisy-systems }\n\n####  Mean Logarithm Squared Error {: #mean-logarithm-squared-error }\n\n* Mean Logarithm Squared Error uses the log function to squash error values and decrease the impact of large errors.\n\n####  Interquartile Mean Absolute Error (IQME) {: #interquartile-mean-absolute-error-iqme }\n\n* By ignoring the smallest 25% and largest 75% of error values, IQME will not be impacted by a significant number of outliers and may work well if you are most interested in \u201con average\u201d performance.\n\n####  Median Absolute Error {: #median-absolute-error }\n\n* By ignoring all residual values except for the median, Median Absolute Error is the most permissive of outliers."", ""link"": ""datarobot_english_documentation/datarobot_docs|en|modeling|reference|eureqa-ref|guidance.txt:1"", ""vector"": [176, 27, 127, 228, 135, 64, 57, 217, 40, 157, 236, 157, 176, 21, 31, 19, 10, 211, 9, 108, 106, 125, 240, 115, 200, 4, 180, 95, 77, 94, 5, 61, 46, 209, 101, 86, 109, 228, 220], ""metadata"": {""chunk_id"": 2723, ""source"": ""datarobot_english_documentation/datarobot_docs|en|modeling|reference|eureqa-ref|guidance.txt"", ""start_index"": 3210, ""similarity_score"": 2.06}}, {""content"": ""response = requests.post(\n        url = DR_APP_HOST + '/api/v2/credentials/',\n        headers=DR_MODELING_HEADERS,\n        json=json\n    )\n\n    if response.status_code == 201:\n\n        return response.json()['credentialId']\n\n    else:\n\n        print('Request failed; http error {code}: {content}'.format(code=response.status_code, content=response.content))\n\n# get or create a credential set\ndef dr_get_or_create_catalog_credentials(name, cred_type, user, password, token=None):\n    cred_id = dr_get_catalog_credentials(name, cred_type)\n\n    if cred_id == None:\n        return dr_create_catalog_credentials(name, cred_type, user, password, token=None)\n    else:\n        return cred_id\n\ncredentials_id = dr_get_or_create_catalog_credentials('snow_community_credentials',\n                                                      'basic', my_creds.SNOW_USER, my_creds.SNOW_PASS)\n```\n\nCreate a session to define the job, which then submits the job and slots it to run asynchronously. DataRobot returns an HTTP 202 status code upon successful submission. You can retrieve the job state by querying the API for the current state of the job.\n\n```python\nsession = requests.Session()\nsession.headers = {\n    'Authorization': 'Bearer {}'.format(API_KEY)\n}\n```\n\nA table to hold the results is created in Snowflake with the following SQL statement, reflecting the structure below:\n\n```python\ncreate or replace TABLE PASSENGERS_SCORED_BATCH_API (\n\tSURVIVED_1_PREDICTION NUMBER(10,9),\n\tSURVIVED_0_PREDICTION NUMBER(10,9),\n\tSURVIVED_PREDICTION NUMBER(38,0),\n\tTHRESHOLD NUMBER(6,5),\n\tPOSITIVE_CLASS NUMBER(38,0),\n\tPASSENGERID NUMBER(38,0)\n);\n```\n\nThe job specifies the following parameters:"", ""link"": ""datarobot_english_documentation/datarobot_docs|en|more-info|how-to|snowflake|sf-server-scoring.txt:0"", ""vector"": [158, 45, 126, 206, 202, 216, 96, 8, 164, 194, 141, 167, 253, 0, 191, 65, 15, 197, 142, 45, 98, 239, 178, 209, 247, 47, 52, 94, 201, 106, 165, 37, 142, 227, 83, 64, 227, 213, 148], ""metadata"": {""chunk_id"": 1880, ""source"": ""datarobot_english_documentation/datarobot_docs|en|more-info|how-to|snowflake|sf-server-scoring.txt"", ""start_index"": -1, ""similarity_score"": 2.05}}, {""content"": ""---\ntitle: Troubleshooting the Python client\ndescription: Review cases that can cause issues with using the Python client and known fixes.\n\n---\n\n# Troubleshooting the Python client {: #troubleshooting-the-python-client }\n\nThis page outlines cases that can cause issues with using the Python client and provides known fixes.\n\n### InsecurePlatformWarning {: #insecureplatformwarning }\n\nPython versions earlier than 2.7.9 might report an [InsecurePlatformWarning](https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning) in your output. To prevent this warning without updating your Python version, you should install the [pyOpenSSL](https://urllib3.readthedocs.org/en/latest/security.html#pyopenssl) package:\n\n`pip install pyopenssl ndg-httpsclient pyasn1`\n\n### AttributeError: 'EntryPoint' object has no attribute 'resolve' {: #attributeerror-entrypoint-object-has-no-attribute-resolve }\n\nSome earlier versions of [setuptools](https://setuptools.pypa.io/en/latest/){ target=_blank } cause an error when importing DataRobot.\n\n```\n>>> import datarobot as dr\n...\nFile \""/home/clark/.local/lib/python2.7/site-packages/trafaret/__init__.py\"", line 1550, in load_contrib\n  trafaret_class = entrypoint.resolve()\nAttributeError: 'EntryPoint' object has no attribute 'resolve'\n```\n\nThe recommended fix is upgrading setuptools to the latest version.\n\n`pip install --upgrade setuptools`\n\nIf you are unable to upgrade, pin [trafaret](https://pypi.python.org/pypi/trafaret/){ target=_blank } to version <=7.4 to correct this issue.\n\n### Connection errors {: #connection-errors }\n\n`configuration.rst` describes how to configure the DataRobot client with the `max_retries` parameter to fine tune\nbehaviors like the number of attempts to retry failed connections.\n\n### ConnectTimeout {: #connecttimeout }\n\nIf you have a slow connection to your DataRobot installation, you may see a traceback like:"", ""link"": ""datarobot_english_documentation/datarobot_docs|en|get-started|gs-get-help|troubleshooting|py-help.txt:0"", ""vector"": [159, 133, 253, 37, 162, 153, 105, 9, 153, 31, 140, 171, 243, 49, 158, 73, 5, 207, 46, 207, 99, 103, 89, 209, 19, 63, 181, 204, 175, 243, 194, 58, 142, 225, 156, 224, 80, 80, 164], ""metadata"": {""chunk_id"": 331, ""source"": ""datarobot_english_documentation/datarobot_docs|en|get-started|gs-get-help|troubleshooting|py-help.txt"", ""start_index"": 0, ""similarity_score"": 1.75}}, {""content"": ""![](images/fraud-claim-12.png)\n\n\n### Features and sample data {: #features-and-sample-data }\n\nThe target variable for this use case is whether or not a claim submitted is fraudulent. It is a binary classification problem. In this dataset 1,746 of 10,746 claims (16%) are fraudulent.\n\nThe target variable:\n\n* `FRAUD`\n\n### Data preparation {: #data-preparation }\n\nBelow are examples of 44 features that can be used to train a model to identify fraud. They consist of historical data on customer policy details, claims data including free-text description, and internal business rules from national databases. These features help DataRobot extract relevant patterns to detect fraudulent claims.\n\nBeyond the features listed below, it might help to incorporate any additional data your organization collects that could be relevant to detecting fraudulent claims. For example, DataRobot is able to process image data as a feature together with numeric, categorical, and text features. Images of vehicles after an accident may be useful to detect fraud and help predict severity.\n\nData from the claim table, policy table, customer table, and vehicle table are merged with customer ID as a key. Only data known before or at the time of the claim creation is used, except for the target variable. Each record in the dataset is a claim.\n\n\n### Sample feature list {: #sample-feature-list }"", ""link"": ""datarobot_english_documentation/datarobot_docs|en|includes|fraud-claims-include.txt:0"", ""vector"": [182, 20, 253, 196, 134, 144, 108, 153, 152, 190, 165, 215, 32, 68, 223, 65, 130, 238, 127, 47, 113, 4, 129, 29, 50, 204, 54, 15, 79, 232, 193, 13, 202, 179, 113, 218, 225, 117, 213], ""metadata"": {""chunk_id"": 1728, ""source"": ""datarobot_english_documentation/datarobot_docs|en|includes|fraud-claims-include.txt"", ""start_index"": 4510, ""similarity_score"": 1.69}}, {""content"": ""To start implementing a custom plugin (`SamplePlugin` below), inherit the `BosunPluginBase` base class. As an example, implement the plugin under `sample_plugin` directory in the file `sample_plugin.py`:\n\n``` python\nclass SamplePlugin(BosunPluginBase):\n    def __init__(self, plugin_config, private_config_file=None, pe_info=None, dry_run=False):\n```\n\n#### Python plugin arguments {: #python-plugin-arguments }\n\nThe constructor is invoked with the following arguments:\n\nArgument | Definition\n---------|-----------\n`plugin_config`       | A dictionary containing general information about the plugin.  We will go over the details in the following section.\n`private_config_file` | Path to the private configuration file for the plugin as passed in by the `--private-config` flag when calling the `bosun-plugin-runner` script. This file is optional and the contents are fully at the discretion of your custom plugin.\n`pe_info`             | An instance of `PEInfo`, which contains information about the prediction environment. This parameter is unset for certain actions.\n`dry_run`             | The invocation for dry run (development) or the actual run.\n\n#### Python plugin methods {: #python-plugin-methods }\n\nThis class implements the following methods:\n\n!!! note\n    The return type for each of the following functions must be `ActionStatusInfo`.\n\n``` python\ndef plugin_start(self):\n```\n\nThis method initializes the plugin; for example, it can check if the plugin can connect with the prediction environment (e.g., Docker, Kubernetes). In the case of the filesystem plugin, this method checks if the `baseDir` exists on the filesystem. Management agent invokes this method typically only once during the startup process.  This method is guaranteed to be called before any deployment-specific action can be invoked.\n\n<hr>\n\n``` python\ndef plugin_stop(self):\n```"", ""link"": ""datarobot_english_documentation/datarobot_docs|en|mlops|deployment|mlops-agent|mgmt-agent|mgmt-agent-plugins.txt:1"", ""vector"": [86, 29, 111, 236, 175, 164, 41, 9, 25, 53, 196, 61, 251, 225, 138, 83, 47, 187, 64, 77, 107, 107, 80, 117, 27, 15, 180, 231, 133, 210, 33, 31, 174, 160, 215, 236, 113, 230, 108], ""metadata"": {""chunk_id"": 734, ""source"": ""datarobot_english_documentation/datarobot_docs|en|mlops|deployment|mlops-agent|mgmt-agent|mgmt-agent-plugins.txt"", ""start_index"": -1, ""similarity_score"": 1.61}}, {""content"": ""The ROC curve's y-axis is True Positives, and the x-axis is False Positives. You can imagine people that drink a lot of wine are ranked on the top right of the curve. They think anything is an alien, so they have a 100% True Positive ranking. They will identify an alien if one exists, but they also have 100% False Positive ranking&mdash;if you say everything is an alien, you're flat out wrong when there really aren't aliens. People ranked on the lower left don't believe in aliens, so nothing is an alien because aliens never existed. They have a 0% False Positive ranking, and 0% True Positive ranking. Again, nothing is an alien, so they will never identify if aliens exist.\n\n    What you want is a person with a 100% True Positive ranking and 0% False Positive ranking&mdash;they correctly identify aliens when they exist, but only when they exist. That's a person that is close to the top-left of the ROC Chart. So your procedure is, take 100 people, and rank them on this space of True Positives vs. False Positives.\n\n    Learn more about [ROC Curve](roc-curve-tab/index).\n\n??? ELI5 \""What is overfitting?\""\n    You tell Goodreads that you like a bunch of Agatha Christie books, and you want to know if you'd like other murder mysteries. It says \u201cno,\u201d because those other books weren't written by Agatha Christie.\n\n    Overfitting is like a bad student who only remembers book facts but does not draw conclusions from them. Any life situation that wasn't specifically mentioned in the book will leave them helpless. But they'll do well on an exam based purely on book facts (that's why you shouldn't score on training data).\n\n??? ELI5 \""Dedicated Prediction Server (DPS) vs. Portable Prediction Server (PPS)\"""", ""link"": ""datarobot_english_documentation/datarobot_docs|en|more-info|eli5.txt:0"", ""vector"": [113, 128, 239, 236, 194, 193, 40, 192, 49, 236, 163, 29, 186, 181, 187, 225, 18, 220, 3, 174, 98, 44, 211, 113, 238, 92, 176, 89, 125, 216, 21, 61, 39, 249, 101, 124, 97, 106, 61], ""metadata"": {""chunk_id"": 699, ""source"": ""datarobot_english_documentation/datarobot_docs|en|more-info|eli5.txt"", ""start_index"": 24901, ""similarity_score"": 1.61}}, {""content"": ""**Request Method:** `POST`\n\n**Request URL:** deployed URL, for example: `https://your-company.orm.datarobot.com/predApi/v1.0`\n\n## Request parameters {: #request-parameters }\n\n### Headers {: #headers }\n\n| Key | Description | Example(s) |\n|------------|----------------|-----|\n| Datarobot-key | A per-organization secret used as an additional authentication factor for prediction servers. Retrieve a datarobot-key pragmatically by accessing\u00a0the `/api/v2/predictionServers/`\u00a0endpoint. The endpoint returns a URL to a prediction server and a corresponding datarobot-key.\u00a0<br><br> Required for Self-Managed AI Platform users; string type  <br><br> Once a model is deployed, see the code snippet in the DataRobot UI, [Predictions > Prediction API](code-py). | `DR-key-12345abcdb-xyz6789` |\n| Authorization  | Required; string <br><br> Three methods are supported: <ul><li>  Bearer authentication  </li><li> (deprecated) Basic authentication: User_email and API token  </li><li> (deprecated) API token | <ul><li>Example for Bearer authentication method: `Bearer API_key-12345abcdb-xyz6789` </li><li>(deprecated) Example for User_email and API token method: `Basic Auth_basic-12345abcdb-xyz6789` </li><li>(deprecated) Example for API token method: `Token API_key-12345abcdb-xyz6789`</li></ul> |\n| Content-Type | Optional; string type |    <ul><li>`text/plain; charset=UTF-8`</li><li>`text/csv`</li><li>`application/json`</li><li>`multipart/form-data` (for files with data, i.e., .csv, .txt files)</ul> |\n| Content-Encoding | Optional; string type <br><br> Currently supports only gzip-encoding with the default data extension. | `gzip` |\n| Accept | Optional; string type <br><br> Controls the shape of the response schema. Currently JSON(default) and CSV are supported. See examples.  |  <ul><li>`application/json` (default)</li><li>`text/csv` (for CSV output)</li></ul> |"", ""link"": ""datarobot_english_documentation/datarobot_docs|en|api|reference|predapi|pred-ref|dep-pred.txt:5"", ""vector"": [222, 73, 115, 68, 170, 148, 81, 88, 140, 166, 132, 31, 49, 146, 159, 65, 47, 125, 135, 69, 226, 199, 179, 245, 55, 175, 133, 127, 72, 102, 165, 170, 204, 129, 254, 224, 105, 125, 204], ""metadata"": {""chunk_id"": 339, ""source"": ""datarobot_english_documentation/datarobot_docs|en|api|reference|predapi|pred-ref|dep-pred.txt"", ""start_index"": -1, ""similarity_score"": 1.6}}, {""content"": ""## Business problem {: #business-problem }\n\nClaim payments and claim adjustment are typically an insurance company\u2019s largest expenses. For long-tail lines of business, such as workers\u2019 compensation (which covers medical expenses and lost wages for injured workers), the true cost of a claim may not be known for many years until it is paid in full. However, claim adjustment activities start when a claim is made aware to the insurer.\n\nTypically when an employee gets injured at work (`Accident Date`), the employer (insured) decides to file a claim to its insurance company (`Report Date`) and a claim record is created in the insurer's claim system with all available information about the claim at the time of reporting. The claim is then assigned to a claim adjuster. This assignment could be purely random or based on roughly defined business rules. During the life cycle of a claim, assignment may be re-evaluated multiple times and re-assigned to a different claim adjuster.\n\nThis process, however, has costly consequences:\n\n* It is well-known in insurance that 20% of claims account for 80% of the total claim payouts. Randomly assigning claims wastes resources.\n\n* Early intervention is critical to optimal claim results. Without the appropriate assignment of resources as early as possible, seemingly mild claims can become substantial.\n\n* Claims of low severity and complexity must wait to be processed alongside all other claims, often leading to a poor customer experience.\n\n* A typical claim adjuster can receive several hundred new claims every month, in addition to any existing open claims. When a claim adjuster is overloaded, it is unlikely they can process every assigned claim. If too much time passes, the claimant is more likely to obtain an attorney to assist in the process, driving up the cost of the claim unnecessarily.\n\n## Solution value {: #solution-value }"", ""link"": ""datarobot_english_documentation/datarobot_docs|en|includes|triage-insurance-claims-include.txt:4"", ""vector"": [39, 141, 237, 118, 130, 147, 233, 218, 144, 142, 241, 118, 147, 194, 191, 105, 136, 206, 207, 141, 107, 79, 240, 147, 48, 132, 182, 5, 137, 187, 193, 37, 138, 211, 119, 61, 242, 49, 141], ""metadata"": {""chunk_id"": 2500, ""source"": ""datarobot_english_documentation/datarobot_docs|en|includes|triage-insurance-claims-include.txt"", ""start_index"": 0, ""similarity_score"": 1.6}}]",False,0.0021343231,False,68131025b8a8acd05b6144a4,15,4a1350cd-3ea8-489f-a25d-c2a29d83e350
